{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Introduction\n",
    "***\n",
    "\n",
    "### Why are non-negative matrices important?\n",
    "\n",
    "Data in many applications is defined on a positive-semidefinite range:\n",
    "\n",
    "- Computer Vision (Pixels defined on $[0,255]$),\n",
    "- Speech recognition (decibels on a scale of $[-96, 0]$ can be transformed),\n",
    "- EEG signals (voltage $[ÂµV]$ defined on $[5, 100]$),\n",
    "- Power consumption ($\\geq 0$),\n",
    "- Frequency (for example in histograms $\\geq 0$),\n",
    "<br>\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Introduction\n",
    "***\n",
    "\n",
    "### How is a non-negative matrix $X$ obtained?\n",
    "\n",
    "<img src=\"img/posMaApp.png\" style=\"float: left;\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Properties of Non-Negative Matrices\n",
    "***\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{f \\times n}$ be a matrix,\n",
    "* the columns of which contain samples and\n",
    "* the rows of which contain features.\n",
    "\n",
    "Hence, the symbol \n",
    "* $f $  denotes the amount of features and\n",
    "* $n $  the amount of observations/samples.\n",
    "\n",
    "Matrix $X_{+}$ is non-negative if all its entries\n",
    "\n",
    "$x_{i,j} \\geq 0,$ $\\forall i, j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Matrix Factorization\n",
    "***\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{f \\times n}_{+}$ be a matrix. \n",
    "\n",
    "Find two matrices $F$ and $W$, so that\n",
    "\n",
    "$X = FW$.\n",
    "\n",
    "---\n",
    "\n",
    "For $F$ and $W$ must hold, that \n",
    "\n",
    "$F \\in \\mathbb{R}^{f \\times r}_{+}$ and $W \\in \\mathbb{R}^{f \\times n}_{+}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Chosen paper\n",
    "***\n",
    "\n",
    "### Factoring nonnegative matrices with linear programs\n",
    "### Bittdorf, Recht et. al.\n",
    "\n",
    "#### Abstract:\n",
    "\n",
    "- Paper presents an approach to factor nonnegative matrices using linear programs\n",
    "- Data-driven approach\n",
    "- Most dominant features are used to represent others\n",
    "- Given a data matrix $X$, a matrix $C$ is identified by the algorithm, so that $X \\approx CX$\n",
    "\n",
    "#### Following slides:\n",
    "\n",
    "- Explenation of the method\n",
    "- Presentation of a simple example\n",
    "- Implementation and verification of solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Example and Theory\n",
    "***\n",
    "Consider a given non-negative data-matrix\n",
    "\n",
    "\\begin{align*}\n",
    "~~~X=\\begin{pmatrix}\n",
    "1&1&0&1&0&1\\\\\n",
    "1&0&2&3&0&1\\\\\n",
    "2&2&0&2&0&2\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "where rows $\\hat{=} $ features ($f=\\#$features), columns $\\hat{=}$  data-points ($n=\\#$data-points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Example and Theory\n",
    "***\n",
    "\\begin{align*}\n",
    "\\textbf{First main idea:} \\text{ Find matrix } C\\in \\mathbb{R}^{3\\times 3} \\text{ such that }X=CX \n",
    "\\end{align*}\n",
    "\n",
    "Since the third row is twice the first one, you can easily determine $C$ as\n",
    "\\begin{align*}\n",
    "C=\\begin{pmatrix}\n",
    "0 & 0 & \\frac{1}{2}\\\\\n",
    "0&1&0 \\\\\n",
    "0&0&1\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "The rows in $X$ corresponding to the diagonal entries of $C$ equal one are called hott topics.\n",
    "Now one has a exact NMF of $X$:\n",
    "\\begin{equation*}\n",
    "X=FW\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Example and Theory\n",
    "***\n",
    "The rows in $X$ corresponding to the diagonal entries of $C$ equal one are called $\\textbf{hott topics}$.\n",
    "\n",
    "Based on the matrix $C$ one construct $F\\in\\mathbb{R}^{f\\times r}$ and $W\\in\\mathbb{R}^{r\\times n}$ by extracting the non-zero columns of $C$ and the hott topics of $X$. In our example one gets\n",
    "\\begin{align*}\n",
    "&X=\\begin{pmatrix}\n",
    "1&1&0&1&0&1\\\\\n",
    "1&0&2&3&0&1\\\\\n",
    "2&2&0&2&0&2\n",
    "\\end{pmatrix} \\rightarrow C=\\begin{pmatrix}\n",
    "0 & 0 & \\frac{1}{2}\\\\\n",
    "0&1&0 \\\\\n",
    "0&0&1\n",
    "\\end{pmatrix}\\\\ \\rightarrow &F= \\begin{pmatrix}\n",
    "0 & \\frac{1}{2} \\\\\n",
    "1&0 \\\\\n",
    "0&1 \n",
    "\\end{pmatrix}~ \\text{ and }~W=\\begin{pmatrix}\n",
    "1&0&2&3&0&1\\\\\n",
    "2&2&0&2&0&2\n",
    "\\end{pmatrix}. \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Example and Theory\n",
    "***\n",
    "<img src=\"img/convex_set.png\" style=\"float: left;\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Example and Theory\n",
    "***\n",
    "In general, determining $C$ is not that easy as in this example. It can be formulated as a linear program:\n",
    "\\begin{equation*}\n",
    "\\textbf{LP to solve:} ~~~\\min_{C\\in \\Phi(X)} p^T \\text{diag}(C)\n",
    "\\end{equation*}\n",
    "where $X$ is a non-negative data-matrix, $p$ is a given price-vector with distinct values and the constraint set is defined as\n",
    "\\begin{equation*}\n",
    "\\Phi\\left(X\\right):=\\lbrace C\\geq \\textbf{0} \\mid CY=Y,~\\text{tr}\\left(C\\right)=r,~C_{jj}\\leq 1~\\forall j,~C_{ij}\\leq C_{jj}~\\forall i,j \\rbrace\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Example and Theory\n",
    "***\n",
    "It is easy to show that this set is convex. By definition of convexity and with $C_1,C_2\\in \\Phi(X)$ and $\\theta\\in \\left[0,1\\right]$ it holds \n",
    "\\begin{align*}\n",
    "C_3&:=\\underbrace{\\theta}_{\\geq 0} \\underbrace{C_1}_{\\geq\\textbf{0}}+(\\underbrace{1-\\theta}_{\\geq 0}) \\underbrace{C_2}_{\\geq\\textbf{0}}\\geq \\textbf{0},\\\\\n",
    "C_3Y&=\\theta \\underbrace{C_1Y}_{=Y}+(1-\\theta)\\underbrace{C_2Y}_{=Y}=Y,\\\\\n",
    "C_{jj}^{(3)}&=\\theta \\underbrace{C_{jj}^{(1)}}_{\\leq 1}+(1-\\theta)\\underbrace{C_{jj}^{(2)}}_{\\leq 1} \\leq \\theta +(1-\\theta)=1~~\\forall j,\\\\\n",
    "C_{ij}^{(3)}&=\\theta \\underbrace{C_{ij}^{(1)}}_{\\leq C_{jj}^{(1)}}+(1-\\theta) \\underbrace{C_{ij}^{(2)}}_{\\leq C_{jj}^{(2)}}\\leq \\theta C_{jj}^{(1)}+(1-\\theta)C_{jj}^{(2)}=C_{jj}^{(3)} ~~\\forall i,j.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Example and Theory\n",
    "***\n",
    "<img src=\"img/algorithm_2.png\" style=\"float: left;\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Example and Theory\n",
    "***\n",
    "$\\textbf{So far:}$ $X$ has an exact separable $r$-rank factorization.\n",
    "\n",
    "$\\textbf{Now:}$ $X$ has an approximately separable $r$-rank factorization, i.e.\n",
    "\\begin{align*}\n",
    "X=Y+\\Delta \n",
    "\\end{align*}\n",
    "where \n",
    "\n",
    "- $Y$ has an exact separable $r$-rank factorization,\n",
    "- $\\Delta$ is an Error-matrix with $\\Vert \\Delta \\Vert_{\\infty,1}\\leq \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Example and Theory\n",
    "***\n",
    "Adjust the constraint set \n",
    "\\begin{equation*}\n",
    "\\Phi_{2\\epsilon}:=\\lbrace C\\geq \\textbf{0} \\mid \\Vert CX-X \\Vert_{\\infty,1}\\leq 2\\epsilon,~\\text{tr}\\left( C \\right)=r,~ C_{jj}\\leq 1~\\forall j,~ C_{ij}\\leq C_{jj}~\\forall i,j  \\rbrace.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Example and Theory\n",
    "***\n",
    "<img src=\"img/algorithm_3.png\" style=\"float: left;\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Implementation: Algorithm 2\n",
    "***\n",
    "<img src=\"img/algorithm_2.png\" style=\"float: left;\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm 2: Separable Nonnegative Matrix Factorization by Linear Programming\n",
      "\n",
      "Input data:\n",
      "\n",
      "Data matrix Y:\n",
      "[[1 1 0 1 0 1]\n",
      " [1 0 2 3 0 1]\n",
      " [2 2 0 2 0 2]] \n",
      "\n",
      "Normalized input data:\n",
      "\n",
      "Data matrix Y_norm:\n",
      "[[0.25       0.25       0.         0.25       0.         0.25      ]\n",
      " [0.14285714 0.         0.28571429 0.42857143 0.         0.14285714]\n",
      " [0.25       0.25       0.         0.25       0.         0.25      ]] \n",
      "\n",
      "Vector p:\n",
      "[[0.]\n",
      " [1.]\n",
      " [2.]] \n",
      "\n",
      "Find matrix C by optimization:\n",
      "--------------\n",
      "\n",
      "Model built.\n",
      "\n",
      "Model solved.\n",
      "\n",
      "--------------\n",
      "\n",
      "Results:\n",
      "\n",
      "Variable C:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]] \n",
      "\n",
      "||Y_norm - C * Y_norm||_inf,1:\n",
      "0.0 \n",
      "\n",
      "--------------\n",
      "\n",
      "Factorization:\n",
      "\n",
      "Matrix W:\n",
      "[[0.25       0.25       0.         0.25       0.         0.25      ]\n",
      " [0.14285714 0.         0.28571429 0.42857143 0.         0.14285714]] \n",
      "\n",
      "Matrix F:\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]] \n",
      "\n",
      "Y_norm = F * W\n",
      "[[0.25       0.25       0.         0.25       0.         0.25      ]\n",
      " [0.14285714 0.         0.28571429 0.42857143 0.         0.14285714]\n",
      " [0.25       0.25       0.         0.25       0.         0.25      ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Algorithm 2: Separable Nonnegative Matrix Factorization by Linear Programming\\n\")\n",
    "\n",
    "## Functions\n",
    "###-----------------------------------------------\n",
    "\n",
    "def distinct_integral_vector( num_values , mode):\n",
    "    output_vector = np.zeros((num_values,1))\n",
    "    for i in range(0,num_values):\n",
    "        if mode == \"ascending\":\n",
    "            output_vector[i] = i\n",
    "        else:\n",
    "            output_vector[i] = num_values - i - 1\n",
    "    return output_vector\n",
    "\n",
    "###-----------------------------------------------\n",
    "\n",
    "def normalize_rows( input_matrix ):\n",
    "    output_matrix = np.zeros((input_matrix.shape[0], input_matrix.shape[1]))\n",
    "    for i in range(0, input_matrix.shape[0]): #For all rows\n",
    "        sum_current_row = 0.0\n",
    "        for j in range(0, input_matrix.shape[1]): #For all columns\n",
    "            sum_current_row += input_matrix[i, j]\n",
    "        for j in range(0, input_matrix.shape[1]): #For all columns\n",
    "            output_matrix[i,j] = input_matrix[i,j]/sum_current_row\n",
    "    return np.matrix(output_matrix)\n",
    "\n",
    "###-----------------------------------------------\n",
    "\n",
    "def build_W_separable( matrix_C, matrix_Y_norm ):\n",
    "    output_matrix_rows = 0\n",
    "    output_matrix_columns = matrix_Y_norm.shape[1]\n",
    "    simplicial_rows = []\n",
    "    \n",
    "    for i in range(0, C.shape[0]):\n",
    "        if C[i,i] == 1.0:\n",
    "            output_matrix_rows += 1\n",
    "            simplicial_rows.append(i)\n",
    "    \n",
    "    output_matrix = np.zeros((output_matrix_rows, output_matrix_columns))\n",
    "    \n",
    "    for i in range(0, len(simplicial_rows)): #New columns\n",
    "        for j in range(0, output_matrix_columns):\n",
    "            output_matrix[i,j] = matrix_Y_norm[simplicial_rows[i], j]\n",
    "            \n",
    "    return output_matrix\n",
    "\n",
    "###-----------------------------------------------\n",
    "\n",
    "def build_F_separable( matrix_C ):\n",
    "    output_matrix_columns = 0\n",
    "    output_matrix_rows = matrix_C.shape[0]\n",
    "    separable_columns = []\n",
    "    \n",
    "    for i in range(0, C.shape[0]):\n",
    "        if C[i,i] == 1.0:\n",
    "            output_matrix_columns += 1\n",
    "            separable_columns.append(i)\n",
    "    \n",
    "    output_matrix = np.zeros((output_matrix_rows, output_matrix_columns))\n",
    "    \n",
    "    for j in range(0, output_matrix_columns): #New columns\n",
    "        for i in range(0, output_matrix_rows): #Row entries\n",
    "            output_matrix[i,j] = matrix_C[i, separable_columns[j]]\n",
    "            \n",
    "    return output_matrix\n",
    "        \n",
    "###-----------------------------------------------\n",
    "\n",
    "# INPUT DATA\n",
    "#-----------\n",
    "\n",
    "r = 2\n",
    "\n",
    "Y = np.matrix([[1,1,0,1,0,1],[1,0,2,3,0,1],[2,2,0,2,0,2]])\n",
    "Y_norm = normalize_rows(Y)\n",
    "Yn = Y_norm\n",
    "\n",
    "f = Y.shape[0]\n",
    "n = Y.shape[1]\n",
    "\n",
    "C = cvx.Variable((f,f), nonneg = True)\n",
    "p = distinct_integral_vector(f, \"ascending\")\n",
    "\n",
    "print(\"Input data:\\n\")\n",
    "print(\"Data matrix Y:\")\n",
    "print(Y,\"\\n\")\n",
    "print(\"Normalized input data:\\n\")\n",
    "print(\"Data matrix Y_norm:\")\n",
    "print(Y_norm,\"\\n\")\n",
    "print(\"Vector p:\")\n",
    "print(p, \"\\n\")\n",
    "print(\"Find matrix C by optimization:\")\n",
    "print(\"--------------\\n\")\n",
    "\n",
    "## Model\n",
    "###-----------------------------------------------\n",
    "\n",
    "objective = cvx.Minimize( p.T * cvx.diag(C))\n",
    "\n",
    "constraints = [C * Y_norm == Y_norm,\n",
    "               cvx.trace(C) == r]\n",
    "\n",
    "for j in range(0,f): # columns\n",
    "    for i in range(0,f): #rows\n",
    "        #constraints += [C[i,j] >= 0]\n",
    "        if j == i:\n",
    "            constraints += [C[i,j] <= 1]\n",
    "        else:\n",
    "            constraints += [C[i, j] <= C[j,j]]\n",
    "\n",
    "prob = cvx.Problem(objective, constraints)#\n",
    "print(\"Model built.\\n\")\n",
    "\n",
    "prob.solve(solver = cvx.GLPK)\n",
    "print(\"Model solved.\\n\")\n",
    "print(\"--------------\\n\")\n",
    "\n",
    "print(\"Results:\\n\")\n",
    "print(\"Variable C:\")\n",
    "C = C.value\n",
    "print(C,\"\\n\")\n",
    "print(\"||Y_norm - C * Y_norm||_inf,1:\")\n",
    "print(np.linalg.norm(C * Y_norm - Y_norm, np.inf), \"\\n\")\n",
    "print(\"--------------\\n\")\n",
    "print(\"Factorization:\\n\")\n",
    "print(\"Matrix W:\")\n",
    "\n",
    "## Factorization\n",
    "###-----------------------------------------------\n",
    "\n",
    "W = build_W_separable(C, Y_norm)\n",
    "print(W, \"\\n\")\n",
    "print(\"Matrix F:\")\n",
    "F = build_F_separable(C)\n",
    "print(F, \"\\n\")\n",
    "print(\"Y_norm = F * W\")\n",
    "print(np.matrix(F) * np.matrix(W),\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Implementation: Algorithm 3\n",
    "***\n",
    "<img src=\"img/algorithm_3.png\" style=\"float: left;\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm 3: Approximably Separable Nonnegative Matrix Factorization by Linear Programming\n",
      "\n",
      "Input data:\n",
      "\n",
      "Data matrix X:\n",
      "[[1 1 0 1 0 1]\n",
      " [1 0 2 3 0 1]\n",
      " [2 2 0 2 0 2]] \n",
      "\n",
      "Normalized input data:\n",
      "\n",
      "Data matrix X_norm:\n",
      "[[0.25       0.25       0.         0.25       0.         0.25      ]\n",
      " [0.14285714 0.         0.28571429 0.42857143 0.         0.14285714]\n",
      " [0.25       0.25       0.         0.25       0.         0.25      ]] \n",
      "\n",
      "Vector p:\n",
      "[[0.]\n",
      " [1.]\n",
      " [2.]] \n",
      "\n",
      "--------------\n",
      "\n",
      "Model built.\n",
      "\n",
      "Model solved.\n",
      "\n",
      "--------------\n",
      "\n",
      "Results:\n",
      "\n",
      "Variable C:\n",
      "[[1.         0.19442079 0.        ]\n",
      " [0.29747247 1.         0.        ]\n",
      " [0.7161661  0.32748827 0.        ]] \n",
      "\n",
      "||X_norm - C * X_norm||_inf,1:\n",
      "0.2974724648000622 \n",
      "\n",
      "Factorization:\n",
      "\n",
      "Matrix W:\n",
      "[[0.25       0.25       0.         0.25       0.         0.25      ]\n",
      " [0.14285714 0.         0.28571429 0.42857143 0.         0.14285714]] \n",
      "\n",
      "Find F by solving an optimization problem:\n",
      "--------------\n",
      "\n",
      "Model built.\n",
      "\n",
      "Model solved.\n",
      "\n",
      "--------------\n",
      "\n",
      "Matrix F:\n",
      "[[ 1.00000000e+00 -5.27719624e-23]\n",
      " [ 1.39105042e-18  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.67109792e-23]] \n",
      "\n",
      "X_norm = F * W\n",
      "[[ 2.50000000e-01  2.50000000e-01 -1.50777035e-23  2.50000000e-01\n",
      "   0.00000000e+00  2.50000000e-01]\n",
      " [ 1.42857143e-01  3.47762605e-19  2.85714286e-01  4.28571429e-01\n",
      "   0.00000000e+00  1.42857143e-01]\n",
      " [ 2.50000000e-01  2.50000000e-01  4.77456550e-24  2.50000000e-01\n",
      "   0.00000000e+00  2.50000000e-01]] \n",
      "\n",
      "||X_norm - F * W||_inf,1:\n",
      "1.6688121629840356e-16 \n",
      "\n",
      "||X_norm - F_dep * W||_inf,1:\n",
      "0.2974724648000622 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Algorithm 3: Approximably Separable Nonnegative Matrix Factorization by Linear Programming\\n\")\n",
    "\n",
    "## Functions\n",
    "###-----------------------------------------------\n",
    "\n",
    "def distinct_integral_vector( num_values , mode):\n",
    "    output_vector = np.zeros((num_values,1))\n",
    "    for i in range(0,num_values):\n",
    "        if mode == \"ascending\":\n",
    "            output_vector[i] = i\n",
    "        else:\n",
    "            output_vector[i] = num_values - i - 1\n",
    "    return output_vector\n",
    "\n",
    "###-----------------------------------------------\n",
    "\n",
    "def normalize_rows( input_matrix ):\n",
    "    output_matrix = np.zeros((input_matrix.shape[0], input_matrix.shape[1]))\n",
    "    for i in range(0, input_matrix.shape[0]): #For all rows\n",
    "        sum_current_row = 0.0\n",
    "        for j in range(0, input_matrix.shape[1]): #For all columns\n",
    "            sum_current_row += input_matrix[i, j]\n",
    "        for j in range(0, input_matrix.shape[1]): #For all columns\n",
    "            output_matrix[i,j] = input_matrix[i,j]/sum_current_row\n",
    "    return np.matrix(output_matrix)\n",
    "\n",
    "###-----------------------------------------------\n",
    "\n",
    "def build_W_nonseparable( matrix_C, matrix_X_norm ):\n",
    "    output_matrix_rows = 0\n",
    "    output_matrix_columns = matrix_X_norm.shape[1]\n",
    "    simplicial_rows = []\n",
    "    \n",
    "    for i in range(0, C.shape[0]):\n",
    "        if abs(matrix_C[i,i] - 1.0) <= 0.001:\n",
    "            output_matrix_rows += 1\n",
    "            simplicial_rows.append(i)\n",
    "    \n",
    "    output_matrix = np.zeros((output_matrix_rows, output_matrix_columns))\n",
    "    \n",
    "    for i in range(0, len(simplicial_rows)): #New columns\n",
    "        for j in range(0, output_matrix_columns):\n",
    "            output_matrix[i,j] = matrix_X_norm[simplicial_rows[i], j]\n",
    "            \n",
    "    return output_matrix\n",
    "\n",
    "###-----------------------------------------------\n",
    "\n",
    "def build_F_nonseparable( matrix_C ):\n",
    "    output_matrix_columns = 0\n",
    "    output_matrix_rows = matrix_C.shape[0]\n",
    "    separable_columns = []\n",
    "    \n",
    "    for i in range(0, C.shape[0]):\n",
    "        if abs(matrix_C[i,i] - 1.0) <= 0.001:\n",
    "            output_matrix_columns += 1\n",
    "            separable_columns.append(i)\n",
    "    \n",
    "    output_matrix = np.zeros((output_matrix_rows, output_matrix_columns))\n",
    "    \n",
    "    for j in range(0, output_matrix_columns): #New columns\n",
    "        for i in range(0, output_matrix_rows): #Row entries\n",
    "            output_matrix[i,j] = matrix_C[i, separable_columns[j]]\n",
    "            \n",
    "    return output_matrix\n",
    "\n",
    "###-----------------------------------------------\n",
    "\n",
    "## Input Data\n",
    "###-----------------------------------------------\n",
    "\n",
    "r = 2\n",
    "\n",
    "X = np.matrix([[1,1,0,1,1,1],[1,0,2,3,0,1],[2,2,0,2,0,2]])\n",
    "X_norm = normalize_rows(Y)\n",
    "\n",
    "f = X.shape[0]\n",
    "n = X.shape[1]\n",
    "\n",
    "C = cvx.Variable((f,f), nonneg = True)\n",
    "p = distinct_integral_vector(f, \"ascending\")\n",
    "\n",
    "print(\"Input data:\\n\")\n",
    "print(\"Data matrix X:\")\n",
    "print(Y,\"\\n\")\n",
    "print(\"Normalized input data:\\n\")\n",
    "print(\"Data matrix X_norm:\")\n",
    "print(Y_norm,\"\\n\")\n",
    "print(\"Vector p:\")\n",
    "print(p, \"\\n\")\n",
    "print(\"--------------\\n\")\n",
    "\n",
    "## Model #1\n",
    "###-----------------------------------------------\n",
    "\n",
    "objective_find_C = cvx.Minimize( p.T * cvx.diag(C) )\n",
    "\n",
    "constraints_find_C = [cvx.norm( C * X_norm - X_norm ) <= 0.3,\n",
    "               cvx.trace(C) == r]\n",
    "\n",
    "for j in range(0,f): # columns\n",
    "    for i in range(0,f): #rows\n",
    "        #constraints += [C[i,j] >= 0]\n",
    "        if j == i:\n",
    "            constraints_find_C += [C[i,j] <= 1]\n",
    "        else:\n",
    "            constraints_find_C += [C[i, j] <= C[j,j]]\n",
    "\n",
    "prob_find_C = cvx.Problem(objective_find_C, constraints_find_C)#\n",
    "print(\"Model built.\\n\")\n",
    "\n",
    "prob_find_C.solve()\n",
    "print(\"Model solved.\\n\")\n",
    "print(\"--------------\\n\")\n",
    "\n",
    "print(\"Results:\\n\")\n",
    "print(\"Variable C:\")\n",
    "C = C.value\n",
    "print(C,\"\\n\")\n",
    "print(\"||X_norm - C * X_norm||_inf,1:\")\n",
    "print(np.linalg.norm(C * X_norm - X_norm, np.inf), \"\\n\")\n",
    "\n",
    "## Factorization #1\n",
    "###-----------------------------------------------\n",
    "\n",
    "print(\"Factorization:\\n\")\n",
    "print(\"Matrix W:\")\n",
    "W = build_W_nonseparable(C, X_norm)\n",
    "print(W, \"\\n\")\n",
    "\n",
    "## Model #2\n",
    "###-----------------------------------------------\n",
    "\n",
    "print(\"Find F by solving an optimization problem:\")\n",
    "print(\"--------------\\n\")\n",
    "\n",
    "F_dep = build_F_nonseparable(C)\n",
    "F = cvx.Variable((Y_norm.shape[0], W.shape[0]))\n",
    "\n",
    "objective_find_F = cvx.Minimize(cvx.norm(X_norm - F * W, \"inf\"))\n",
    "\n",
    "prob_find_F = cvx.Problem(objective_find_F)\n",
    "print(\"Model built.\\n\")\n",
    "\n",
    "prob_find_F.solve()\n",
    "print(\"Model solved.\\n\")\n",
    "print(\"--------------\\n\")\n",
    "print(\"Matrix F:\")\n",
    "print(F.value, \"\\n\")\n",
    "print(\"X_norm = F * W\")\n",
    "print(np.matrix(F.value) * W,\"\\n\")\n",
    "\n",
    "print(\"||X_norm - F * W||_inf,1:\")\n",
    "print(np.linalg.norm(X_norm - np.matrix(F.value) * W, np.inf), \"\\n\")\n",
    "\n",
    "print(\"||X_norm - F_dep * W||_inf,1:\")\n",
    "print(np.linalg.norm(X_norm - np.matrix(F_dep) * W, np.inf), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Conclusion\n",
    "***\n",
    "\n",
    "#### Advantages and Disadvantages:\n",
    "\n",
    "- No assumptions that features follow a certain function (unlike linear regression)\n",
    "- Assumed that features lie in one convex hull\n",
    "- Assumptions regarding parameters $r$ and $\\epsilon$\n",
    "\n",
    "#### Application:\n",
    "\n",
    "- Dimensionality reduction\n",
    "- Efficient for large data sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Thanks for your attention. Questions?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Backup Slides\n",
    "***\n",
    "<img src=\"img/algorithm_4.png\" style=\"float: left;\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Non-Negative Matrix Factorization\n",
    "___\n",
    "\n",
    "## Backup Slides\n",
    "***\n",
    "<img src=\"img/error.png\" style=\"float: left;\" width=\"800\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
